<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aura Voice Client</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 600px;
            margin: 50px auto;
            padding: 20px;
            background: #1e1e1e;
            color: #fff;
        }
        h1 {
            color: #61dafb;
        }
        .container {
            background: #2d2d2d;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
        }
        .input-group {
            margin-bottom: 20px;
        }
        label {
            display: block;
            margin-bottom: 5px;
            color: #aaa;
        }
        input {
            width: 100%;
            padding: 10px;
            border: 1px solid #444;
            border-radius: 5px;
            background: #1e1e1e;
            color: #fff;
            font-size: 14px;
            box-sizing: border-box;
        }
        button {
            width: 100%;
            padding: 15px;
            font-size: 16px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            margin-bottom: 10px;
            transition: background 0.3s;
        }
        .connect-btn {
            background: #61dafb;
            color: #000;
        }
        .connect-btn:hover {
            background: #4fa8c5;
        }
        .connect-btn:disabled {
            background: #444;
            cursor: not-allowed;
        }
        .disconnect-btn {
            background: #f44336;
            color: #fff;
            display: none;
        }
        .disconnect-btn:hover {
            background: #d32f2f;
        }
        .status {
            padding: 15px;
            border-radius: 5px;
            margin-top: 20px;
            text-align: center;
        }
        .status.disconnected {
            background: #444;
            color: #aaa;
        }
        .status.connecting {
            background: #ff9800;
            color: #000;
        }
        .status.connected {
            background: #4caf50;
            color: #fff;
        }
        .instructions {
            background: #333;
            padding: 15px;
            border-radius: 5px;
            margin-top: 20px;
            font-size: 14px;
            line-height: 1.6;
        }
        .instructions h3 {
            margin-top: 0;
            color: #61dafb;
        }
        code {
            background: #1e1e1e;
            padding: 2px 6px;
            border-radius: 3px;
        }
        .transcript-box {
            background: #333;
            padding: 20px;
            border-radius: 5px;
            margin-top: 20px;
        }
        .transcript-line {
            color: #61dafb;
            margin-bottom: 8px;
        }
        .transcript-line .time {
            color: #888;
            font-size: 12px;
            margin-right: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ Aura Voice Client</h1>

        <div class="input-group">
            <label for="tokenUrl">Token Server URL:</label>
            <input type="text" id="tokenUrl" placeholder="http://172.25.18.244:3000/token" value="http://172.25.18.244:3000/token">
        </div>

        <div class="input-group">
            <label for="room">Room Name:</label>
            <input type="text" id="room" placeholder="demo-chat" value="demo-chat">
        </div>

        <div class="input-group">
            <label for="identity">Your Identity:</label>
            <input type="text" id="identity" placeholder="phone-user" value="phone-user">
        </div>

        <button class="connect-btn" id="connectBtn" onclick="connect()">Connect & Start Voice</button>
        <button class="disconnect-btn" id="disconnectBtn" onclick="disconnect()">Disconnect</button>
        <button class="connect-btn" onclick="testAudio()" style="background: #ff9800; margin-top: 10px;">Test Audio (Beep)</button>

        <div class="status disconnected" id="status">
            Disconnected
        </div>

        <div class="transcript-box" id="transcriptBox" style="display: none;">
            <h3 style="margin-top: 0; color: #61dafb;">Live Transcript:</h3>
            <div id="transcriptContent" style="min-height: 100px; max-height: 300px; overflow-y: auto; background: #1e1e1e; padding: 15px; border-radius: 5px; font-family: monospace; line-height: 1.6;">
                <div style="color: #888;">Waiting for speech...</div>
            </div>
        </div>

        <div class="instructions">
            <h3>Instructions:</h3>
            <ol>
                <li>Make sure backend is running: <code>cd backend && npm run dev</code></li>
                <li>Make sure token server is running: <code>cd backend/token-server && npm start</code></li>
                <li>Make sure codingterminal is running: <code>cd codingterminal && ./aura.sh ws://localhost:8765</code></li>
                <li>Click "Connect & Start Voice"</li>
                <li>Allow microphone access</li>
                <li>Say <strong>"Hey Aura"</strong> to activate</li>
                <li>Ask your coding question!</li>
            </ol>
        </div>
    </div>

    <script type="module">
        import { Room } from 'https://esm.sh/livekit-client@2.5.8';

        let room = null;
        let transcriptWs = null;

        // Create AudioContext once (iOS WebView compatible)
        let audioContext = null;

        function getAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }
            // Resume context if suspended (iOS requirement)
            if (audioContext.state === 'suspended') {
                audioContext.resume();
            }
            return audioContext;
        }

        function updateStatus(message, className) {
            const statusEl = document.getElementById('status');
            statusEl.textContent = message;
            statusEl.className = 'status ' + className;
        }

        function addTranscript(text) {
            const transcriptBox = document.getElementById('transcriptBox');
            const transcriptContent = document.getElementById('transcriptContent');

            // Show the transcript box
            transcriptBox.style.display = 'block';

            // Clear "Waiting for speech..." message
            if (transcriptContent.querySelector('[style*="color: #888"]')) {
                transcriptContent.innerHTML = '';
            }

            // Highlight wake and stop words
            console.log('üîç Processing transcript for highlighting:', text);
            let highlightedText = text;
            let hasWake = false;
            let hasStop = false;

            // Check for wake words (case insensitive, flexible with punctuation)
            const wakePatterns = [
                { pattern: /(hey[,\s]*aura[?!,.]?)/gi, color: '#4caf50' },
                { pattern: /(hi[,\s]*aura[?!,.]?)/gi, color: '#4caf50' },
                { pattern: /(hello[,\s]*aura[?!,.]?)/gi, color: '#4caf50' },
                { pattern: /(yo[,\s]*aura[?!,.]?)/gi, color: '#4caf50' },
                { pattern: /(ok[,\s]*aura[?!,.]?)/gi, color: '#4caf50' }
            ];

            for (const {pattern, color} of wakePatterns) {
                if (pattern.test(text)) {
                    console.log('‚úÖ Found wake word match:', pattern);
                    highlightedText = text.replace(pattern, `<span style="background: ${color}; color: #000; padding: 2px 6px; border-radius: 3px; font-weight: bold;">$1</span>`);
                    hasWake = true;
                    break;
                }
            }

            // Check for stop words (case insensitive, flexible with punctuation)
            const stopPatterns = [
                { pattern: /(bye[,\s]*aura[?!,.]?)/gi, color: '#f44336' },
                { pattern: /(bye[,\s]*oro[?!,.]?)/gi, color: '#f44336' },
                { pattern: /(stop[,\s]*aura[?!,.]?)/gi, color: '#f44336' },
                { pattern: /(bye[!,.]?)/gi, color: '#f44336' },
                { pattern: /(goodbye[!,.]?)/gi, color: '#f44336' },
                { pattern: /(cancel[!,.]?)/gi, color: '#f44336' },
                { pattern: /(nevermind[!,.]?)/gi, color: '#f44336' }
            ];

            if (!hasWake) {
                for (const {pattern, color} of stopPatterns) {
                    if (pattern.test(text)) {
                        console.log('‚úÖ Found stop word match:', pattern);
                        highlightedText = highlightedText.replace(pattern, `<span style="background: ${color}; color: #fff; padding: 2px 6px; border-radius: 3px; font-weight: bold;">$1</span>`);
                        hasStop = true;
                        break;
                    }
                }
            }

            console.log('üé® Highlighting result - hasWake:', hasWake, 'hasStop:', hasStop);

            // Add new transcript line with timestamp
            const time = new Date().toLocaleTimeString();
            const line = document.createElement('div');
            line.className = 'transcript-line';

            let prefix = '';
            if (hasWake) prefix = 'üü¢ ';
            if (hasStop) prefix = 'üî¥ ';

            line.innerHTML = `<span class="time">[${time}]</span> ${prefix}${highlightedText}`;
            transcriptContent.appendChild(line);

            // Auto-scroll to bottom
            transcriptContent.scrollTop = transcriptContent.scrollHeight;
        }

        let reconnectAttempts = 0;
        const maxReconnectAttempts = 10;
        let reconnectTimeout = null;

        function connectToTranscriptStream() {
            // Clear any existing reconnect timeout
            if (reconnectTimeout) {
                clearTimeout(reconnectTimeout);
                reconnectTimeout = null;
            }

            // Connect to backend WebSocket to receive transcripts and audio
            const wsUrl = 'ws://172.25.18.244:8765';

            try {
                transcriptWs = new WebSocket(wsUrl);

                transcriptWs.onopen = () => {
                    console.log('‚úì Connected to transcript stream');
                    reconnectAttempts = 0; // Reset counter on successful connection
                };

                transcriptWs.onmessage = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        console.log('üìã Message type:', data.type);

                        // Handle transcript display
                        if ((data.type === 'query' || data.type === 'transcript') && data.content) {
                            console.log('‚úÖ Adding transcript:', data.content);
                            addTranscript(data.content);
                        }
                        // Handle audio playback
                        else if (data.type === 'audio' && data.content) {
                            console.log('üîä Received audio data, size:', data.content.length);
                            playAudio(data.content).catch(err => {
                                console.error('Audio playback failed:', err);
                            });
                        }
                    } catch (error) {
                        console.error('Failed to parse message:', error);
                    }
                };

                transcriptWs.onerror = (error) => {
                    console.error('Transcript WebSocket error:', error);
                };

                transcriptWs.onclose = () => {
                    console.log('Transcript stream disconnected');

                    // Attempt to reconnect
                    if (reconnectAttempts < maxReconnectAttempts) {
                        reconnectAttempts++;
                        const delay = Math.min(1000 * reconnectAttempts, 5000);
                        console.log(`üîÑ Reconnecting in ${delay}ms (attempt ${reconnectAttempts}/${maxReconnectAttempts})`);
                        reconnectTimeout = setTimeout(connectToTranscriptStream, delay);
                    } else {
                        console.error('‚ùå Max reconnection attempts reached');
                        updateStatus('Connection lost. Please refresh.', 'disconnected');
                    }
                };
            } catch (error) {
                console.error('Failed to create WebSocket:', error);
            }
        }

        async function playAudio(base64Audio) {
            console.log('üéµ Playing audio...');
            updateStatus('üîä Aura is speaking...', 'connected');
            try {
                // Convert base64 to ArrayBuffer
                const audioData = atob(base64Audio);
                const arrayBuffer = new ArrayBuffer(audioData.length);
                const view = new Uint8Array(arrayBuffer);
                for (let i = 0; i < audioData.length; i++) {
                    view[i] = audioData.charCodeAt(i);
                }

                console.log('üìä Audio size:', arrayBuffer.byteLength, 'bytes');

                // Use Web Audio API (works better in iOS WebView)
                const ctx = getAudioContext();
                console.log('AudioContext state:', ctx.state);
                console.log('AudioContext sample rate:', ctx.sampleRate);

                // Decode WAV file
                console.log('üîÑ Attempting to decode audio...');
                const audioBuffer = await ctx.decodeAudioData(arrayBuffer);
                console.log('‚úÖ Audio decoded successfully!');
                console.log('üìä Audio duration:', audioBuffer.duration, 'seconds');
                console.log('üìä Sample rate:', audioBuffer.sampleRate);
                console.log('üìä Channels:', audioBuffer.numberOfChannels);

                // Create source and connect to destination
                const source = ctx.createBufferSource();
                source.buffer = audioBuffer;

                // Create gain node for volume control
                const gainNode = ctx.createGain();
                gainNode.gain.value = 1.0;

                source.connect(gainNode);
                gainNode.connect(ctx.destination);

                // Play audio
                source.start(0);
                console.log('‚úÖ Audio playing via Web Audio API!');

                // Clean up when finished
                source.onended = () => {
                    console.log('üéµ Audio finished');
                    updateStatus('‚úì Connected! Say "Hey Aura" to start', 'connected');
                };

            } catch (error) {
                console.error('‚ùå Failed to play audio:', error);
                console.error('Error details:', error.name, error.message);
                updateStatus('‚ùå Audio failed: ' + error.message, 'disconnected');

                // Try playing a simple beep as fallback
                console.log('üîî Attempting fallback beep...');
                try {
                    const ctx = getAudioContext();
                    const oscillator = ctx.createOscillator();
                    const gainNode = ctx.createGain();

                    oscillator.connect(gainNode);
                    gainNode.connect(ctx.destination);

                    oscillator.frequency.value = 440;
                    oscillator.type = 'sine';
                    gainNode.gain.setValueAtTime(0.3, ctx.currentTime);

                    oscillator.start(ctx.currentTime);
                    oscillator.stop(ctx.currentTime + 0.3);

                    console.log('‚úÖ Fallback beep attempted');
                } catch (beepError) {
                    console.error('‚ùå Even fallback beep failed:', beepError);
                }
            }
        }

        window.connect = async function() {
            const tokenUrl = document.getElementById('tokenUrl').value;
            const roomName = document.getElementById('room').value;
            const identity = document.getElementById('identity').value;

            const connectBtn = document.getElementById('connectBtn');
            const disconnectBtn = document.getElementById('disconnectBtn');

            try {
                // CRITICAL: Initialize AudioContext on user gesture (iOS requirement)
                const ctx = getAudioContext();
                console.log('üîä AudioContext initialized on user gesture. State:', ctx.state);
                if (ctx.state === 'suspended') {
                    await ctx.resume();
                    console.log('üîä AudioContext resumed. New state:', ctx.state);
                }

                updateStatus('Fetching token...', 'connecting');
                connectBtn.disabled = true;

                // Fetch token
                const url = `${tokenUrl}?room=${roomName}&identity=${identity}`;
                console.log('Fetching token from:', url);

                const response = await fetch(url);
                const data = await response.json();

                console.log('Token response:', data);

                if (!data.token) {
                    throw new Error('No token received');
                }

                updateStatus('Connecting to LiveKit...', 'connecting');

                // Connect to LiveKit
                room = new Room();

                // Listen for remote audio tracks (TTS responses)
                room.on('trackSubscribed', (track, publication, participant) => {
                    console.log('üì¢ Track subscribed:', track.kind, 'from', participant.identity);
                    console.log('üì¢ Track source:', publication.source);
                    console.log('üì¢ Track name:', publication.trackName);

                    if (track.kind === 'audio') {
                        console.log('üîä Attaching audio track to element');
                        const audioElement = track.attach();
                        audioElement.id = 'tts-audio-' + Date.now();
                        audioElement.autoplay = true;
                        audioElement.volume = 1.0;
                        audioElement.muted = false;
                        audioElement.setAttribute('playsinline', '');

                        document.body.appendChild(audioElement);

                        // Monitor audio data
                        let audioStarted = false;
                        const checkAudio = setInterval(() => {
                            if (audioElement.currentTime > 0 && !audioElement.paused) {
                                if (!audioStarted) {
                                    console.log('üéµ AUDIO IS PLAYING! Time:', audioElement.currentTime);
                                    audioStarted = true;
                                }
                            }
                        }, 100);

                        // Force play
                        audioElement.play().then(() => {
                            console.log('‚úÖ Audio element playing successfully');
                            console.log('   Volume:', audioElement.volume);
                            console.log('   Muted:', audioElement.muted);
                            console.log('   Paused:', audioElement.paused);
                            console.log('   Duration:', audioElement.duration);
                            console.log('   ReadyState:', audioElement.readyState);
                        }).catch(err => {
                            console.error('‚ùå Failed to play audio:', err);
                            clearInterval(checkAudio);
                        });

                        // Clean up on track end
                        track.on('ended', () => {
                            console.log('üéµ Audio track ended');
                            clearInterval(checkAudio);
                        });

                        // Monitor for errors
                        audioElement.onerror = (e) => {
                            console.error('‚ùå Audio element error:', e);
                            clearInterval(checkAudio);
                        };

                        // Monitor when audio actually starts playing
                        audioElement.onplaying = () => {
                            console.log('üéµ Audio onplaying event fired');
                        };

                        audioElement.onended = () => {
                            console.log('üéµ Audio ended');
                            clearInterval(checkAudio);
                        };
                    }
                });

                room.on('trackUnsubscribed', (track) => {
                    console.log('Track unsubscribed:', track.kind);
                    track.detach().forEach(el => el.remove());
                });

                await room.connect(data.url, data.token);

                updateStatus('Connected! Checking microphone...', 'connecting');

                // Try to enable microphone (optional - not required for phone-based input)
                try {
                    await room.localParticipant.setMicrophoneEnabled(true);
                    console.log('Microphone enabled');

                    // Get the microphone track
                    const tracks = Array.from(room.localParticipant.audioTrackPublications.values());
                    console.log('Audio tracks published:', tracks.length);

                    if (tracks.length === 0) {
                        console.warn('No audio track published from computer (using phone mic instead)');
                    }
                } catch (micError) {
                    console.warn('Computer microphone not available (using phone mic):', micError.message);
                }

                updateStatus('‚úì Connected! Say "Hey Aura" to start', 'connected');

                connectBtn.style.display = 'none';
                disconnectBtn.style.display = 'block';

                // Connect to transcript stream to see what you're saying
                connectToTranscriptStream();

                console.log('Connected to room:', roomName);
                console.log('Room participants:', room.remoteParticipants.size);

            } catch (error) {
                console.error('Connection failed:', error);
                updateStatus('Connection failed: ' + error.message, 'disconnected');
                connectBtn.disabled = false;
            }
        }

        window.disconnect = function() {
            if (room) {
                room.disconnect();
                room = null;
            }

            const connectBtn = document.getElementById('connectBtn');
            const disconnectBtn = document.getElementById('disconnectBtn');

            connectBtn.style.display = 'block';
            connectBtn.disabled = false;
            disconnectBtn.style.display = 'none';

            updateStatus('Disconnected', 'disconnected');
            console.log('Disconnected from room');
        }

        window.testAudio = function() {
            console.log('üîä Testing audio with beep...');
            try {
                // Use same audio context
                const ctx = getAudioContext();
                console.log('AudioContext state:', ctx.state);

                // Create a simple 440Hz beep for 0.5 seconds
                const oscillator = ctx.createOscillator();
                const gainNode = ctx.createGain();

                oscillator.connect(gainNode);
                gainNode.connect(ctx.destination);

                oscillator.frequency.value = 440; // A4 note
                oscillator.type = 'sine';

                gainNode.gain.setValueAtTime(0.3, ctx.currentTime);
                gainNode.gain.exponentialRampToValueAtTime(0.01, ctx.currentTime + 0.5);

                oscillator.start(ctx.currentTime);
                oscillator.stop(ctx.currentTime + 0.5);

                console.log('‚úÖ Beep played! If you heard it, audio is working.');
                alert('Test beep played! Did you hear it?');
            } catch (error) {
                console.error('‚ùå Audio test failed:', error);
                alert('Audio test failed: ' + error.message);
            }
        }
    </script>
</body>
</html>
